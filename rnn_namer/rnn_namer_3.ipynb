{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNN起名器（三）—— 程序实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "通过前面的两篇博客，我们已经获取了训练数据和字向量，还了解了RNN单元的原理和代码实现。\n",
    "这篇博客继续讲解如何实现一个RNN起名器(使用LSTM)。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 网络结构\n",
    "\n",
    "先看下RNN网络常用的基础结构，图片来自[karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)：\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/nlp_projects/master/resources/diags.jpeg)\n",
    "\n",
    "解释：\n",
    "\n",
    "- (1) 简单的一对一。（严格的说不属于RNN） \n",
    "- (2) 序列输出 (例如输入一张图片，输出一句句子)。 \n",
    "- (3) 序列输入 (例如输入一句话，做情感分类)。 \n",
    "- (4) 序列输入，序列输出 (典型例子：Machine Translation)。 \n",
    "- (5) 同步的序列输入和输出 (例如视频分类，给视频的每一帧画面打label)。\n",
    "\n",
    "我们的起名器使用的是最后一种同步序列输入和输出。\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/nlp_projects/master/resources/charseq.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. lstm最终实现\n",
    "\n",
    "[上一篇](https://applenob.github.io/rnn_2.html)介绍了lstm的基本实现。\n",
    "\n",
    "接下来，我们看下我们的最终实现：\n",
    "\n",
    "```python\n",
    "with self.graph.as_default():\n",
    "    # Parameters:\n",
    "    # Embedding layer\n",
    "    with tf.name_scope(\"embedding\"):\n",
    "        self.Vector = tf.Variable(initial_value=self.W_value, name=\"Vector\")\n",
    "    # input to all gates\n",
    "    U = tf.Variable(tf.truncated_normal([self.embedding_dim, self.hidden_dim * 4], -0.1, 0.1), name='x')\n",
    "    # memory of all gates\n",
    "    W = tf.Variable(tf.truncated_normal([self.hidden_dim, self.hidden_dim * 4], -0.1, 0.1), name='m')\n",
    "    # biases all gates\n",
    "    biases = tf.Variable(tf.zeros([1, self.hidden_dim * 4]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([self.batch_size, self.hidden_dim]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([self.batch_size, self.hidden_dim]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([self.hidden_dim, self.vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "    self.keep_prob = tf.placeholder(tf.float32, name=\"kb\")\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        i = tf.nn.dropout(x=i, keep_prob=self.keep_prob)\n",
    "        mult = tf.matmul(i, U) + tf.matmul(o, W) + biases\n",
    "        input_gate = tf.sigmoid(mult[:, :self.hidden_dim])\n",
    "        forget_gate = tf.sigmoid(mult[:, self.hidden_dim:self.hidden_dim * 2])\n",
    "        update = mult[:, self.hidden_dim * 3:self.hidden_dim * 4]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(mult[:, self.hidden_dim * 3:])\n",
    "        output = tf.nn.dropout(output_gate * tf.tanh(state), self.keep_prob)\n",
    "        return output, state\n",
    "```\n",
    "\n",
    "上面的代码把iU，fU，cU，oU堆叠成U，把iW，fW，cW，oW堆叠成W。\n",
    "\n",
    "这样，矩阵乘法：\n",
    "```python\n",
    "tf.matmul(i, iU) + tf.matmul(o, iW) + ib\n",
    "tf.matmul(i, fU) + tf.matmul(o, fW) + fb\n",
    "tf.matmul(i, cU) + tf.matmul(o, cW) + cb\n",
    "tf.matmul(i, oU) + tf.matmul(o, oW) + ob\n",
    "```\n",
    "就可以合成下面的一步：\n",
    "```python\n",
    "mult = tf.matmul(i, U) + tf.matmul(o, W) + biases\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. mini-batch\n",
    "\n",
    "如果不使用mini-batch，一个一个样本训练，速度会很慢。\n",
    "\n",
    "为了加快训练速度，RNN通常也会采用mini-batch的方式训练。\n",
    "\n",
    "但是问题来了，不同的训练语句长度不一样怎么办？一般采用固定batch长度，不够的zero padding补上；多出的分割成多个。\n",
    "\n",
    "下面的代码生成batch数据：\n",
    "\n",
    "```python\n",
    "class BatchGenerator(object):\n",
    "    \"\"\"Batch 生成器\"\"\"\n",
    "    def __init__(self, X_value, Y_value, batch_size,\n",
    "                 num_unrollings, vocabulary_size, char_to_index):\n",
    "        self.X_value = X_value\n",
    "        self.Y_value = Y_value\n",
    "        self.data_len = len(X_value)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_unrollings = num_unrollings\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.char_to_index = char_to_index\n",
    "        self.start = 0\n",
    "        self.end = batch_size - 1\n",
    "\n",
    "        print \"data length:\", len(X_value)\n",
    "\n",
    "    def next(self):\n",
    "        X_all = self.X_value[[i % self.data_len for i in range(self.start, self.end + 1)]]\n",
    "        Y_all = self.Y_value[[i % self.data_len for i in range(self.start, self.end + 1)]]\n",
    "        X_all = [x + list(np.zeros(self.num_unrollings - len(x), dtype=int)) for x in X_all if len(x) != self.num_unrollings]\n",
    "        Y_all = [y + list(np.zeros(self.num_unrollings - len(y), dtype=int)) for y in Y_all if len(y) != self.num_unrollings]\n",
    "        X_batchs = list()\n",
    "        Y_batchs = list()\n",
    "        for step in range(self.num_unrollings):\n",
    "            X_batch = list()\n",
    "            Y_batch = np.zeros(shape=(self.batch_size, self.vocabulary_size), dtype=np.float)\n",
    "            for b in range(self.batch_size):\n",
    "                X_batch.append(X_all[b][step])\n",
    "                Y_batch[b, Y_all[b][step]] = 1.0\n",
    "            X_batchs.append(np.array(X_batch))\n",
    "            Y_batchs.append(Y_batch)\n",
    "        self.start = self.end + 1\n",
    "        self.end += self.batch_size\n",
    "        return X_batchs, Y_batchs\n",
    "\n",
    "```\n",
    "\n",
    "因为要使用字向量，所以X_batch数据是字的index(根据index查询char embedding)，而Y_batch数据是one hot向量。\n",
    "\n",
    "所以X_batchs的尺寸是：(5, 50)，即num_unrollings×batch_size；\n",
    "\n",
    "Y_batchs的尺寸是：(5, 50, 5273)，即num_unrollings×batch_size×num_chars。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.损失函数和模型评估\n",
    "\n",
    "**损失函数**：根据softmax的输出和label计算交叉熵\n",
    "\n",
    "```python\n",
    "logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "self.loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, self.train_labels)))\n",
    "```\n",
    "\n",
    "**评估指标**：perplexity\n",
    "\n",
    "perplexity 衡量概率模型的采样的有多好，数值越小，概率模型越好（语言模型常用）。\n",
    "\n",
    "$$PPL = \\sqrt[N]{\\frac{1}{P(w_1,w_2,...w_N))}}$$\n",
    "\n",
    "$$= e^{\\frac{1}{N}\\boldsymbol{ln}\\frac{1}{P(w_1,w_2,...,w_N)}}$$\n",
    "\n",
    "$$=e^{-\\frac{1}{N}\\sum ^N_{i=1}\\boldsymbol{ln}P(w_i)}$$\n",
    "\n",
    "```python\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"\n",
    "    计算perplexity时用到。\n",
    "    Log-probability of the true labels in a predicted batch.\n",
    "    \"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "```\n",
    "\n",
    "```python\n",
    "print('Minibatch perplexity: %.2f' % float(\n",
    "    np.exp(logprob(predictions, np.concatenate(Y_batchs)))))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 生成名字（sample）\n",
    "\n",
    "```python\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    sample按照distribution的概率分布采样下标，这里的采样方式是针对离散的分布，相当于连续分布中求CDF。\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction, vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\n",
    "    根据sample_distribution采样得的下标值，转换成1-hot的样本\n",
    "    \"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "```\n",
    "```python\n",
    "def sample_name(self, first_name, ckpt_file=MODEL_PRE):\n",
    "    \"\"\"根据现有模型，sample生成名字\"\"\"\n",
    "    with tf.Session(graph=self.graph) as session:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(session, ckpt_file)\n",
    "        for _ in range(NAME_NUM):\n",
    "            name = first_name\n",
    "            sample_input = self.char_to_index[first_name[-1]]\n",
    "            self.reset_sample_state.run()\n",
    "            for _ in range(NAME_LEN-1):\n",
    "                prediction = self.sample_prediction.eval({self.sample_input: [sample_input], self.keep_prob: 1.0})\n",
    "                one_hot = sample(prediction, self.vocabulary_size)\n",
    "                sample_input = self.char_to_index[prob_to_char(one_hot, self.index_to_char)[0]]\n",
    "                name += prob_to_char(one_hot, self.index_to_char)[0]\n",
    "            print name\n",
    "```\n",
    "\n",
    "根据输入的姓，和名字长度，获取名字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 结果展示\n",
    "\n",
    "完整[代码](https://github.com/applenob/pick_a_name/blob/master/vi_lstm_c2v.py)\n",
    "\n",
    "训练模型在main函数中执行train_all()\n",
    "\n",
    "生成名字在main函数中执行namer_lstm_c2v()\n",
    "\n",
    "生成的陈姓男孩名字：\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/nlp_projects/master/resources/name.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of part 3\n"
     ]
    }
   ],
   "source": [
    "print \"end of part 3\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
